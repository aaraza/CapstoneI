{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be used when converted to real script to maintain ID ordering when we cluster and label \n",
    "# just need to change target table \n",
    "\n",
    "conn = psycopg2.connect(\"dbname='cap' user='postgres' host='ec2-52-27-114-159.us-west-2.compute.amazonaws.com' port=9000 password ='secret'\")\n",
    "data = pd.read_sql_query(\"SELECT * FROM nlp_dim ORDER BY id ASC LIMIT 100\", conn)\n",
    "\n",
    "# data = pd.read_csv('nlp_dim_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 20 columns):\n",
      "index                 100 non-null int64\n",
      "site                  100 non-null object\n",
      "title                 100 non-null object\n",
      "author                100 non-null object\n",
      "published_on          100 non-null object\n",
      "accessed_on           100 non-null object\n",
      "url                   100 non-null object\n",
      "body                  100 non-null object\n",
      "newspaper_keywords    100 non-null object\n",
      "newspaper_summary     100 non-null object\n",
      "id                    100 non-null int64\n",
      "tokenized_body        100 non-null object\n",
      "word_count            100 non-null int64\n",
      "stopworded_body       100 non-null object\n",
      "lemmatized_body       100 non-null object\n",
      "word_bag              100 non-null object\n",
      "named_entities        100 non-null object\n",
      "lexical_diversity     100 non-null float64\n",
      "sentiment_score       100 non-null object\n",
      "binary_sentiment      100 non-null int64\n",
      "dtypes: float64(1), int64(4), object(15)\n",
      "memory usage: 15.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/tf_vectorizer_obj.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "# transforms data into tfidf matrix representation\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=100,\n",
    "                                 min_df=2, use_idf=True)\n",
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "\n",
    "joblib.dump(vectorizer, 'model/tf_vectorizer_obj.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x100 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3667 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit our data (list of article bodies) to a tfidf representation\n",
    "X = vectorizer.fit_transform(data.lemmatized_body)\n",
    "\n",
    "# verify we have a sparse matrix of 100 tfidf features for each article \n",
    "# should be 5*100 sparse matrix\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the data that we have of TFIDF vectors into a file\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse.save_npz('model/tf_idf.npz', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x100 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3667 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sparse.load_npz('model/tf_idf.npz')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# How many clusters we want\n",
    "true_k = 3\n",
    "\n",
    "# create the KMeans object with initial settings\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 0.000\n",
      "Converged at iteration 0: center shift 7.813979e-31 within tolerance 6.586000e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=3, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit our tfidf data to the kmeans model\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '44', 'around', 'award', 'back', 'bar', 'belfast', 'best', 'black', 'bt1', 'build', 'but', 'caption', 'ceiling', 'cellars', 'center', 'church', 'co', 'cocktail', 'crawl', 'create', 'crown', 'district', 'drink', 'duke', 'first', 'five', 'food', 'garrick', 'get', 'good', 'guinness', 'hand', 'hide', 'host', 'hotel', 'in', 'include', 'international', 'ireland', 'irish', 'island', 'jack', 'keith', 'kelly', 'lane', 'like', 'live', 'london', 'lot', 'maddens', 'mcgarry', 'mean', 'menu', 'merchant', 'muldoon', 'museum', 'music', 'newly', 'northern', 'offer', 'open', 'owner', 'palmer', 'parliament', 'pc', 'photo', 'picture', 'pint', 'pub', 'pubs', 'rabbit', 'room', 'run', 'sale', 'sean', 'shamrock', 'shop', 'short', 'sort', 'spaniard', 'spot', 'st', 'starred', 'stuff', 'take', 'thing', 'tour', 'tourist', 'traditional', 'travel', 'try', 'turf', 'watch', 'welcome', 'well', 'when', 'whiskey', 'york', 'young']\n",
      " palmer keith pc young hotel\n",
      "\n",
      " belfast photo best pub bar\n",
      "\n",
      " london watch parliament hotel five\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save kmeans model \n",
    "joblib.dump(km, 'model/kmeans_model.pkl')\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "labels = km.labels_\n",
    "print(terms)\n",
    "\n",
    "# order_centroids\n",
    "\n",
    "for i in range(3):\n",
    "    for ind in order_centroids[i, :5]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " palmer keith pc young hotel\n",
      "\n",
      " belfast photo best pub bar\n",
      "\n",
      " london watch parliament hotel five\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare saved and loaded kmeans\n",
    "kmeans_loaded = joblib.load('model/kmeans_model.pkl')\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "order_centroids = kmeans_loaded.cluster_centers_.argsort()[:, ::-1]\n",
    "labels = kmeans_loaded.labels_\n",
    "\n",
    "# order_centroids\n",
    "\n",
    "for i in range(3):\n",
    "    for ind in order_centroids[i, :5]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small test for how we can eventually persist the cluster labels for individual articles\n",
    "# Since the labels attribute is in the order that the sparse matrix was in when it was passed in\n",
    "# We should be able just insert the label value as a dataframe column\n",
    "\n",
    "t = pd.Series(labels)\n",
    "data['cluster_label'] = t\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=100)\n",
    "X_test = tfidf.fit_transform([data.lemmatized_body[98]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "z = km.predict(X_test)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
