{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_index(low_index, high_index):\n",
    "    \"\"\"\n",
    "    Updates low and high index\n",
    "    \"\"\"\n",
    "    file = open(\"data.txt\", \"w\")\n",
    "    file.writelines(str(data['max']+1) + \"\\n\")\n",
    "    file.writelines(str(data['max']+1000) + \"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_index():\n",
    "    \"\"\"\n",
    "    We want to process 1000 articles at a time\n",
    "    This reads the low id and high id from a text file and returns a dict that can be passed to the postgress query\n",
    "    \"\"\"\n",
    "    raw_data = [int(line.rstrip('\\n')) for line in open(\"data.txt\")]\n",
    "    data = dict()\n",
    "    data['min'] = raw_data[0]\n",
    "    data['max'] = raw_data[1]\n",
    "    data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f02cb8459aa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtokenized_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbody\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtokenized_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import re\n",
    "from __future__ import division\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "data = read_index()\n",
    "conn = psycopg2.connect(\"dbname='cap' user='postgres' host='ec2-52-27-114-159.us-west-2.compute.amazonaws.com' port=9000 password ='secret'\")\n",
    "df = pd.read_sql_query(sql=\"SELECT * FROM articles WHERE ID >=%(min)s and ID <= %(max)s ORDER BY ID\", con=conn, params=data)\n",
    "\n",
    "tokenized_body = []\n",
    "for body in df['body']:\n",
    "    body = body.decode('utf-8')\n",
    "    tokens = nltk.word_tokenize(body)\n",
    "    tokenized_body.append(tokens)\n",
    "\n",
    "se = pd.Series(tokenized_body)\n",
    "df['tokenized_body'] = se.values\n",
    "\n",
    "word_count = []\n",
    "for body in df['tokenized_body']:\n",
    "    word_count.append(len(body))\n",
    "\n",
    "se = pd.Series(word_count)\n",
    "df['word_count'] = se.values\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + [',', '.', '!', '?', '\"','\\'', '/', '\\\\', '-', '--', 'â€”', '(', ')', '[', ']', '\\'s', '\\'t', '\\'ve', '\\'d', '\\'ll', '\\'re']\n",
    "stop_words = set(stop_words) # making this a set increases performance for large documents\n",
    "\n",
    "stopworded_body = []\n",
    "for body in df['tokenized_body']:\n",
    "    stopworded_body.append([w.lower() for w in body if w not in stop_words])\n",
    "\n",
    "se = pd.Series(stopworded_body)\n",
    "df['stopworded_body'] = se.values\n",
    "\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "lemmatized_body = []\n",
    "for body in df['stopworded_body']:\n",
    "    # We need to tag words with their parts of speech before the WordNet lemmatizer will work properly\n",
    "    pos_tagged_body = nltk.pos_tag(body)\n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos_tagged_body:\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        if not wntag:\n",
    "            lemma = word\n",
    "        else:\n",
    "            lemma = wnl.lemmatize(word, wntag)\n",
    "        lemmatized_words.append(lemma)\n",
    "    lemmatized_body.append(lemmatized_words)\n",
    "\n",
    "se = pd.Series(lemmatized_body)\n",
    "df['lemmatized_body'] = se.values\n",
    "\n",
    "word_bag = []\n",
    "for body in df['lemmatized_body']:\n",
    "    fdist = FreqDist(body)\n",
    "    # FreqDist returns a special nltk.probability.FreqDist type\n",
    "    # This is a list of tuples\n",
    "    word_bag.append(fdist.most_common())\n",
    "\n",
    "se = pd.Series(word_bag)\n",
    "df['word_bag'] = se.values\n",
    "\n",
    "st = StanfordNERTagger('/media/justin/Data/Google Drive/Assignments and Projects/Machine Learning/NLP/english.all.3class.distsim.crf.ser.gz',\n",
    "\t\t\t\t\t   '/media/justin/Data/Google Drive/Assignments and Projects/Machine Learning/NLP/stanford-ner.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')\n",
    "\n",
    "classified_texts = []\n",
    "for body in df['tokenized_body']:\n",
    "    classified_texts.append(st.tag(body))\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def stanfordNE2BIO(tagged_sent):\n",
    "    bio_tagged_sent = []\n",
    "    prev_tag = \"O\"\n",
    "    for token, tag in tagged_sent:\n",
    "        if tag == \"O\": #O\n",
    "            bio_tagged_sent.append((token, tag))\n",
    "            prev_tag = tag\n",
    "            continue\n",
    "        if tag != \"O\" and prev_tag == \"O\": # Begin NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag == tag: # Inside NE\n",
    "            bio_tagged_sent.append((token, \"I-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag != tag: # Adjacent NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "\n",
    "    return bio_tagged_sent\n",
    "\n",
    "def stanfordNE2tree(ne_tagged_sent):\n",
    "    bio_tagged_sent = stanfordNE2BIO(ne_tagged_sent)\n",
    "    sent_tokens, sent_ne_tags = zip(*bio_tagged_sent)\n",
    "    sent_pos_tags = [pos for token, pos in pos_tag(sent_tokens)]\n",
    "\n",
    "    sent_conlltags = [(token, pos, ne) for token, pos, ne in zip(sent_tokens, sent_pos_tags, sent_ne_tags)]\n",
    "    ne_tree = conlltags2tree(sent_conlltags)\n",
    "    return ne_tree\n",
    "\n",
    "ne_trees = []\n",
    "for text in classified_texts:\n",
    "    try:\n",
    "        ne_trees.append(stanfordNE2tree(text))\n",
    "    except:\n",
    "        ne_trees.append(' ')\n",
    "\n",
    "ne_in_sent = []\n",
    "ne_in_sents = []\n",
    "for tree in ne_trees:\n",
    "    ne_in_sent = []\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == Tree: # If subtree is a noun chunk, i.e. NE != \"O\"\n",
    "            ne_label = subtree.label()\n",
    "            ne_string = \" \".join([token for token, pos in subtree.leaves()])\n",
    "            ne_in_sent.append((ne_string, ne_label))\n",
    "    ne_in_sents.append(ne_in_sent)\n",
    "\n",
    "se = pd.Series(ne_in_sents)\n",
    "df['named_entities'] = se.values\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text) * 100\n",
    "\n",
    "lex_div = []\n",
    "for body in df['stopworded_body']:\n",
    "    lex_div.append(lexical_diversity(body))\n",
    "\n",
    "se = pd.Series(lex_div)\n",
    "df['lexical_diversity'] = se.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis using VaderSentiment https://github.com/cjhutto/vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a87fb4b47f8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0msentiment_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mbinary_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_sentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a87fb4b47f8b>\u001b[0m in \u001b[0;36mcalculate_sentiment\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# returns a list of sentiment scores for each article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mbody_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentence_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0msid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0msentiment_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a87fb4b47f8b>\u001b[0m in \u001b[0;36mget_sentence_tokens\u001b[0;34m(df_body_column)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbody_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbody\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_body_column\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbody_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# We need to create sentence-based tokens of the articles to do sentiment analysis\n",
    "# pass this function the 'body' column from our article database and it will \n",
    "# return a list the articles in the form of a list of sentences\n",
    "def get_sentence_tokens(df_body_column):\n",
    "    body_sentences = []\n",
    "    for body in df_body_column:\n",
    "        body = body.decode('utf-8')\n",
    "        sentences = sent_tokenize(body)\n",
    "        body_sentences.append(sentences)\n",
    "    return body_sentences\n",
    "# Our sentiment analysis process is based on find a sentiment analysis score for \n",
    "# each sentence in an article so to get the average score for an article\n",
    "# we need to divide each individual sum by the number of sentences in the article\n",
    "def average_sentiment(neg, neu, pos, compound, length):\n",
    "    result = {}\n",
    "    result['neg'] = neg/length\n",
    "    result['neu'] = neu/length\n",
    "    result['pos'] = pos/length\n",
    "    result['compound'] = compound/length\n",
    "    return result\n",
    "\n",
    "# just pass in the dataframe of our database\n",
    "# returns a list of sentiment scores for each article\n",
    "def calculate_sentiment(df):\n",
    "    body_sentences = get_sentence_tokens(df['body'])\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = []\n",
    "\n",
    "    for text in body_sentences:\n",
    "        neg = 0\n",
    "        neu = 0\n",
    "        pos = 0\n",
    "        compound = 0\n",
    "        try:\n",
    "            for sent in text:\n",
    "                ss = sid.polarity_scores(sent)\n",
    "                neg += ss['neg']\n",
    "                neu += ss['neu']\n",
    "                pos += ss['pos']\n",
    "                compound += ss['compound']\n",
    "            result = average_sentiment(neg, neu, pos, compound, len(text))\n",
    "            sentiment_score.append(result)\n",
    "        except:\n",
    "            sentiment_score.append({'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0})\n",
    "    return sentiment_score\n",
    "\n",
    "# pass in the list of sentiment scores for our articles to assign a binary positive or negative value\n",
    "def assign_sentiment(score_list):\n",
    "    binary_sentiment = []\n",
    "    for score in score_list:\n",
    "        if score['compound'] > 0:\n",
    "            binary_sentiment.append(1)\n",
    "        else:\n",
    "            binary_sentiment.append(0)\n",
    "    return binary_sentiment\n",
    "\n",
    "sentiment_score = calculate_sentiment(df)\n",
    "binary_sentiment = assign_sentiment(sentiment_score)\n",
    "print(binary_sentiment)\n",
    "\n",
    "se = pd.Series(sentiment_score)\n",
    "df['sentiment_score'] = se.values\n",
    "se = pd.Series(binary_sentiment)\n",
    "df['binary_sentiment'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgresql://postgres:secret@ec2-52-27-114-159.us-west-2.compute.amazonaws.com:9000/cap')\n",
    "df.to_sql(name='nlp_dim', con=engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END OF NLP / START OF ORIGINAL DW CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'site', u'title', u'author', u'secondary_authors', u'published_on',\n",
       "       u'accessed_on', u'url', u'body', u'html', u'newspaper_keywords',\n",
       "       u'newspaper_summary', u'id', u'tokenized_body', u'word_count',\n",
       "       u'stopworded_body', u'lemmatized_body', u'word_bag', u'named_entities',\n",
       "       u'lexical_diversity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname='cap' user='postgres' host='ec2-35-163-99-253.us-west-2.compute.amazonaws.com' port=9000 password ='secret'\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('drop table if exists author_dim cascade')\n",
    "cursor.execute('create table author_dim(author_id SERIAL,name TEXT,PRIMARY KEY(author_id))')\n",
    "\n",
    "cursor.execute('drop table if exists site_dim cascade')\n",
    "cursor.execute('create table site_dim(site_id SERIAL,domain text,url text,supersite_id int,PRIMARY KEY(site_id))')\n",
    "\n",
    "cursor.execute('drop table if exists article_dim cascade')\n",
    "cursor.execute('create table article_dim(article_id SERIAL, title text, published_date timestamp,accessed_date timestamp, body text,html text, keywords text, summary text,PRIMARY KEY(article_id))')\n",
    "\n",
    "cursor.execute('drop table if exists statistic_dim cascade')\n",
    "cursor.execute('create table statistic_dim(statistic_id SERIAL,NLP_results JSON,PRIMARY KEY(statistic_id))')\n",
    "\n",
    "cursor.execute('drop table if exists fact cascade')\n",
    "cursor.execute('create table fact(fact_id SERIAL,article_id int,author_id int,site_id int,statistic_id int,FOREIGN KEY (article_id) REFERENCES article_dim(article_id),FOREIGN KEY (author_id) REFERENCES author_dim(author_id),FOREIGN KEY (site_id) REFERENCES site_dim(site_id),FOREIGN KEY (statistic_id) REFERENCES statistic_dim(statistic_id))')\n",
    "\n",
    "cursor.execute('drop table if exists nlp_dim cascade')\n",
    "cursor.execute(\"create table nlp_dim(article_id SERIAL,tokenized_body text, word_count integer, stopworded_body text, lemmatized_body text, word_bag text, named_entities text, lexical_diversity float, PRIMARY KEY(article_id))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "article_table = df[['id','title', 'published_on','accessed_on', 'body', 'html','newspaper_keywords', 'newspaper_summary']]\n",
    "article_table.accessed_on = article_table.accessed_on[0].split(' ')[0]\n",
    "article_table.columns = ['article_id','title', 'published_date','accessed_date', 'body', 'html','keywords', 'summary']\n",
    "article_table\n",
    "#engine = create_engine('postgresql://postgres:secret@ec2-35-163-99-253.us-west-2.compute.amazonaws.com:9000/cap')\n",
    "#article_table.to_sql('article_dim', engine, if_exists='append', index=None)\n",
    "# df[['id','tokenized_body', 'word_count','stopworded_body', 'lemmatized_body','word_bag','named_entities','lexical_diversity']].to_sql('nlp_dim', engine, if_exists='append', index=None)\n",
    "# cursor.execute('select * from article_dim limit 1')\n",
    "# cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp_table = df[['id','tokenized_body', 'word_count','stopworded_body', 'lemmatized_body','word_bag','named_entities','lexical_diversity']]\n",
    "nlp_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp.to_sql('nlp_dim', engine, if_exists='append', index=None)\n",
    "cursor.execute('select * from nlp_dim limit 1')\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conn = psycopg2.connect(\"dbname='cap' user='postgres' host='ec2-35-163-99-253.us-west-2.compute.amazonaws.com' port=9000 password ='secret'\")\n",
    "# cursor = conn.cursor()\n",
    "# cursor.execute('drop table if exists author_dim cascade')\n",
    "# cursor.execute('create table author_dim(author_id SERIAL,name TEXT,PRIMARY KEY(author_id))')\n",
    "\n",
    "# cursor.execute('drop table if exists site_dim cascade')\n",
    "# cursor.execute('create table site_dim(site_id SERIAL,domain text,url text,supersite_id int,PRIMARY KEY(site_id))')\n",
    "\n",
    "# cursor.execute('drop table if exists article_dim cascade')\n",
    "# cursor.execute('create table article_dim(article_id SERIAL, title text, published_date timestamp,accessed_date timestamp, body text,html text, keywords text, summary text,PRIMARY KEY(article_id))')\n",
    "\n",
    "# cursor.execute('drop table if exists statistic_dim cascade')\n",
    "# cursor.execute('create table statistic_dim(statistic_id SERIAL,NLP_results JSON,PRIMARY KEY(statistic_id))')\n",
    "\n",
    "# cursor.execute('drop table if exists fact cascade')\n",
    "# cursor.execute('create table fact(fact_id SERIAL,article_id int,author_id int,site_id int,statistic_id int,FOREIGN KEY (article_id) REFERENCES article_dim(article_id),FOREIGN KEY (author_id) REFERENCES author_dim(author_id),FOREIGN KEY (site_id) REFERENCES site_dim(site_id),FOREIGN KEY (statistic_id) REFERENCES statistic_dim(statistic_id))')\n",
    "\n",
    "# cursor.execute('drop table if exists nlp_dim cascade')\n",
    "cursor.execute(\"select * from nlp_dim limit 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_table = pd.DataFrame(data=None, columns=['author_id', 'author'], index = None)\n",
    "author_table.author = data['author'].unique()\n",
    "author_table.author_id = author_table.index \n",
    "author_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_tableMM = data[['author', 'id']]\n",
    "author_tableMM.columns = ['author', 'article_id']\n",
    "author_tableMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.merge(author_table, author_tableMM, on='author')\n",
    "result[result.article_id==2910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_table[author_table.author_id==507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s= data['url'][0]\n",
    "#print(s)\n",
    "s = s.replace(\"http://www.\",\"\")\n",
    "#print(s)\n",
    "domain = s.rsplit('/')[0]\n",
    "#print(domain)\n",
    "supersite = domain.rsplit('.')[0]\n",
    "print(s, domain, supersite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site_table = pd.DataFrame(data=data[['id','url']], columns=['id','site_id', 'url', 'domain', 'supersite'], index = None)\n",
    "site_table.url = data['url']\n",
    "site_table.url = site_table.url.str.replace(\"http://www.\",\"\")\n",
    "site_table.domain = site_table.url.apply(lambda x: pd.Series(str(x).split('/',1)))\n",
    "site_table.supersite = site_table.domain.apply(lambda x: pd.Series(str(x).split('.',1)))\n",
    "site_table.site_id = site_table.index\n",
    "site_table.columns = ['article_id','site_id', 'URL', 'domain', 'supersite']\n",
    "site_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result1 = pd.merge(author_tableMM, author_table, on='author')[['article_id','author_id']]\n",
    "result2 = pd.merge(site_table, result1, on='article_id')[['article_id','author_id','site_id']]\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import re\n",
    "pattern = re.compile('(^[^.]*[.])([^.]*)([.].*$)')\n",
    "site_table = pd.DataFrame(data=data[['id','url']], columns=['id','site_id', 'url', 'domain', 'supersite'], index = None)\n",
    "site_table.url = data['url']\n",
    "#site_table.url =  urlparse(site_table.url).netloc\n",
    "for x in site_table.url:\n",
    "    x = urlparse(x).netloc\n",
    "\n",
    "\n",
    "'''site_table.domain = site_table.url.apply(lambda x: pd.Series(str(x).split('/',1)))\n",
    "site_table.supersite = site_table.domain.apply(lambda x: pd.Series(str(x).split('.',1)))\n",
    "site_table.site_id = site_table.index\n",
    "site_table.columns = ['article_id','site_id', 'URL', 'domain', 'supersite']\n",
    "site_table'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import re\n",
    "pattern = re.compile('(^[^.]*[.])([^.]*)([.].*$)')\n",
    "a = urlparse(data.url[0]).netloc\n",
    "print(a)\n",
    "b = [pattern.sub(r'\\2', x) for x in [a]][0]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = urlparse(data.url[0])\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
