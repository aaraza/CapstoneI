{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import re\n",
    "from __future__ import division\n",
    "from nltk.tag import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname='cap' user='postgres' host='ec2-35-163-99-253.us-west-2.compute.amazonaws.com' port=9000 password ='secret'\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT * FROM articles limit 1000\")\n",
    "data = cursor.fetchall()\n",
    "df = pd.DataFrame(data)\n",
    "data.columns = ['site', 'title','author','secondary_authors','published_on','accessed_on','url','body' ,'html','newspaper_keywords','newspaper_summary','id']                 \n",
    "#print(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# conn = psycopg2.connect(\"dbname='cap' user='postgres' host='ec2-35-163-99-253.us-west-2.compute.amazonaws.com' port=9000 password ='secret'\")\n",
    "# df = pd.read_sql_query(\"SELECT * FROM article_dim limit 5\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published_date</th>\n",
       "      <th>accessed_date</th>\n",
       "      <th>body</th>\n",
       "      <th>html</th>\n",
       "      <th>keywords</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23136</td>\n",
       "      <td>Bill O’Reilly: ‘Completely unfounded claims’ c...</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-04-19</td>\n",
       "      <td>2017-04-21 08:50:09.665591</td>\n",
       "      <td>Bill O’Reilly called the claims that resulted ...</td>\n",
       "      <td>&lt;div&gt;&lt;p class=\"T(0) Pos(a) Start(0) End(0) B(0...</td>\n",
       "      <td>{oreilly,fox,departure,caused,allegations,bill...</td>\n",
       "      <td>Bill O’Reilly called the claims that resulted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23137</td>\n",
       "      <td>Meghan Markle \"Breezes Through Security\" at Ke...</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-04-20</td>\n",
       "      <td>2017-04-21 08:50:14.825553</td>\n",
       "      <td>From Marie Claire  Even though Pippa remains s...</td>\n",
       "      <td>&lt;div&gt;&lt;p class=\"canvas-atom canvas-text Mb(1.0e...</td>\n",
       "      <td>{markle,know,royal,security,wedding,vote,palac...</td>\n",
       "      <td>From Marie ClaireEven though Pippa remains sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23138</td>\n",
       "      <td>California's 'weed nuns' on a mission to heal ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-04-20</td>\n",
       "      <td>2017-04-21 08:50:27.262492</td>\n",
       "      <td>By Omar Younis  MERCED, Calif. (Reuters) - The...</td>\n",
       "      <td>&lt;div&gt;&lt;p class=\"canvas-atom canvas-text Mb(1.0e...</td>\n",
       "      <td>{californias,trump,heal,cannabis,marijuana,mis...</td>\n",
       "      <td>By Omar YounisMERCED, Calif. (Reuters) - The S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23139</td>\n",
       "      <td>Jennifer Hudson dances with legendary producer...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-04-21 08:50:39.785140</td>\n",
       "      <td>View gallery Jennifer Hudson dances with legen...</td>\n",
       "      <td>&lt;div&gt;&lt;p id=\"yom-ad-LREC-iframe\" class=\"yom-ad\"...</td>\n",
       "      <td>{clive,houston,producer,whitney,hudson,legenda...</td>\n",
       "      <td>View gallery Jennifer Hudson dances with legen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23140</td>\n",
       "      <td>Hooters Altering Iconic Uniforms for More 'Fam...</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-04-20</td>\n",
       "      <td>2017-04-21 08:50:59.625593</td>\n",
       "      <td>Family friendly isn't exactly the first thing ...</td>\n",
       "      <td>&lt;div&gt;&lt;p class=\"T(0) Pos(a) Start(0) End(0) B(0...</td>\n",
       "      <td>{uniforms,friendly,altering,outlet,skirts,girl...</td>\n",
       "      <td>SEE ALSO: Hooters Ball Girl Accidentally Ruins...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title   url  \\\n",
       "0       23136  Bill O’Reilly: ‘Completely unfounded claims’ c...  None   \n",
       "1       23137  Meghan Markle \"Breezes Through Security\" at Ke...  None   \n",
       "2       23138  California's 'weed nuns' on a mission to heal ...  None   \n",
       "3       23139  Jennifer Hudson dances with legendary producer...  None   \n",
       "4       23140  Hooters Altering Iconic Uniforms for More 'Fam...  None   \n",
       "\n",
       "  published_date              accessed_date  \\\n",
       "0     2017-04-19 2017-04-21 08:50:09.665591   \n",
       "1     2017-04-20 2017-04-21 08:50:14.825553   \n",
       "2     2017-04-20 2017-04-21 08:50:27.262492   \n",
       "3            NaT 2017-04-21 08:50:39.785140   \n",
       "4     2017-04-20 2017-04-21 08:50:59.625593   \n",
       "\n",
       "                                                body  \\\n",
       "0  Bill O’Reilly called the claims that resulted ...   \n",
       "1  From Marie Claire  Even though Pippa remains s...   \n",
       "2  By Omar Younis  MERCED, Calif. (Reuters) - The...   \n",
       "3  View gallery Jennifer Hudson dances with legen...   \n",
       "4  Family friendly isn't exactly the first thing ...   \n",
       "\n",
       "                                                html  \\\n",
       "0  <div><p class=\"T(0) Pos(a) Start(0) End(0) B(0...   \n",
       "1  <div><p class=\"canvas-atom canvas-text Mb(1.0e...   \n",
       "2  <div><p class=\"canvas-atom canvas-text Mb(1.0e...   \n",
       "3  <div><p id=\"yom-ad-LREC-iframe\" class=\"yom-ad\"...   \n",
       "4  <div><p class=\"T(0) Pos(a) Start(0) End(0) B(0...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  {oreilly,fox,departure,caused,allegations,bill...   \n",
       "1  {markle,know,royal,security,wedding,vote,palac...   \n",
       "2  {californias,trump,heal,cannabis,marijuana,mis...   \n",
       "3  {clive,houston,producer,whitney,hudson,legenda...   \n",
       "4  {uniforms,friendly,altering,outlet,skirts,girl...   \n",
       "\n",
       "                                             summary  \n",
       "0  Bill O’Reilly called the claims that resulted ...  \n",
       "1  From Marie ClaireEven though Pippa remains sta...  \n",
       "2  By Omar YounisMERCED, Calif. (Reuters) - The S...  \n",
       "3  View gallery Jennifer Hudson dances with legen...  \n",
       "4  SEE ALSO: Hooters Ball Girl Accidentally Ruins...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_body = []\n",
    "for body in df['body']:\n",
    "    body = body.decode('utf-8')\n",
    "    tokens = nltk.word_tokenize(body)\n",
    "    tokenized_body.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "se = pd.Series(tokenized_body)\n",
    "df['tokenized_body'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = []\n",
    "for body in df['tokenized_body']:\n",
    "    word_count.append(len(body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "se = pd.Series(word_count)\n",
    "df['word_count'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + [',', '.', '!', '?', '\"','\\'', '/', '\\\\', '-', '--', '—', '(', ')', '[', ']', '\\'s', '\\'t', '\\'ve', '\\'d', '\\'ll', '\\'re']\n",
    "stop_words = set(stop_words) # making this a set increases performance for large documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>secondary_authors</th>\n",
       "      <th>published_on</th>\n",
       "      <th>accessed_on</th>\n",
       "      <th>url</th>\n",
       "      <th>body</th>\n",
       "      <th>html</th>\n",
       "      <th>newspaper_keywords</th>\n",
       "      <th>newspaper_summary</th>\n",
       "      <th>id</th>\n",
       "      <th>tokenized_body</th>\n",
       "      <th>word_count</th>\n",
       "      <th>stopworded_body</th>\n",
       "      <th>lemmatized_body</th>\n",
       "      <th>word_bag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NBC</td>\n",
       "      <td>O.J. Simpson to Receive Parole Hearing in July...</td>\n",
       "      <td>Andrew Blankstein</td>\n",
       "      <td>['Daniella Silva']</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-05-23 09:33:29.288780</td>\n",
       "      <td>http://www.nbcnews.com/news/us-news/o-j-simpso...</td>\n",
       "      <td>After nearly a decade behind bars, O.J. Simpso...</td>\n",
       "      <td>&lt;div&gt;&lt;p&gt; After nearly a decade behind bars, O....</td>\n",
       "      <td>{simpson,board,charges,nevada,oj,prison,receiv...</td>\n",
       "      <td>Simpson is set to face a parole hearing this J...</td>\n",
       "      <td>75718</td>\n",
       "      <td>[After, nearly, a, decade, behind, bars, ,, O....</td>\n",
       "      <td>438</td>\n",
       "      <td>[after, nearly, decade, behind, bars, o.j, sim...</td>\n",
       "      <td>[after, nearly, decade, behind, bar, o.j, simp...</td>\n",
       "      <td>[(parole, 12), (simpson, 11), (charge, 5), (bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NBC</td>\n",
       "      <td>Justice Department Narrows Possible Sanctions ...</td>\n",
       "      <td>Pete Williams</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-05-23 09:33:45.267153</td>\n",
       "      <td>http://www.nbcnews.com/news/us-news/justice-de...</td>\n",
       "      <td>Attorney General Jeff Sessions said Monday tha...</td>\n",
       "      <td>&lt;div&gt;&lt;p&gt; Attorney General Jeff Sessions said M...</td>\n",
       "      <td>{order,possible,justice,narrows,sanctions,secu...</td>\n",
       "      <td>Attorney General Jeff Sessions said Monday tha...</td>\n",
       "      <td>75719</td>\n",
       "      <td>[Attorney, General, Jeff, Sessions, said, Mond...</td>\n",
       "      <td>366</td>\n",
       "      <td>[attorney, general, jeff, sessions, said, mond...</td>\n",
       "      <td>[attorney, general, jeff, session, say, monday...</td>\n",
       "      <td>[(local, 6), (federal, 6), (city, 6), (session...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NBC</td>\n",
       "      <td>Daughter Pleads for Help in Search for Missing...</td>\n",
       "      <td>Rachael Trost</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-05-23 09:33:54.960824</td>\n",
       "      <td>http://www.nbcnews.com/feature/missing-in-amer...</td>\n",
       "      <td>Clarissa is Lisa Harvey's only child.  They tw...</td>\n",
       "      <td>&lt;div&gt;&lt;p&gt; Clarissa is Lisa Harvey's only child....</td>\n",
       "      <td>{lake,woman,phone,daughter,told,missing,lisas,...</td>\n",
       "      <td>Lisa Harvey Help Find Lisa Gail Harvey Faceboo...</td>\n",
       "      <td>75720</td>\n",
       "      <td>[Clarissa, is, Lisa, Harvey, 's, only, child, ...</td>\n",
       "      <td>991</td>\n",
       "      <td>[clarissa, lisa, harvey, child, they, two, inc...</td>\n",
       "      <td>[clarissa, lisa, harvey, child, they, two, inc...</td>\n",
       "      <td>[(lisa, 26), (clarissa, 18), (say, 14), ('', 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NBC</td>\n",
       "      <td>Alabama Death Row Inmate Tommy Arthur Pleads f...</td>\n",
       "      <td>Tracy Connor</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-05-23 09:34:07.816292</td>\n",
       "      <td>http://www.nbcnews.com/storyline/lethal-inject...</td>\n",
       "      <td>Seven times, Tommy Arthur has escaped death. W...</td>\n",
       "      <td>&lt;div&gt;&lt;p&gt; Seven times, Tommy Arthur has escaped...</td>\n",
       "      <td>{row,inmate,court,wicker,guilty,testing,arthur...</td>\n",
       "      <td>Seven times, Tommy Arthur has escaped death.\\n...</td>\n",
       "      <td>75721</td>\n",
       "      <td>[Seven, times, ,, Tommy, Arthur, has, escaped,...</td>\n",
       "      <td>1221</td>\n",
       "      <td>[seven, times, tommy, arthur, escaped, death, ...</td>\n",
       "      <td>[seven, time, tommy, arthur, escape, death, wi...</td>\n",
       "      <td>[(arthur, 27), (``, 17), ('', 16), (say, 15), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NBC</td>\n",
       "      <td>Did Ford Nix Its CEO Because He Was Too Much o...</td>\n",
       "      <td>Paul A. Eisenstein</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-05-23 09:34:47.090816</td>\n",
       "      <td>http://www.nbcnews.com/business/autos/ford-s-f...</td>\n",
       "      <td>Former Ford CEO Mark Fields fashioned himself ...</td>\n",
       "      <td>&lt;div&gt;&lt;p&gt; Former Ford CEO Mark Fields fashioned...</td>\n",
       "      <td>{visionary,hackett,vision,nix,ceo,chairman,for...</td>\n",
       "      <td>Former Ford CEO Mark Fields fashioned himself ...</td>\n",
       "      <td>75722</td>\n",
       "      <td>[Former, Ford, CEO, Mark, Fields, fashioned, h...</td>\n",
       "      <td>1247</td>\n",
       "      <td>[former, ford, ceo, mark, fields, fashioned, a...</td>\n",
       "      <td>[former, ford, ceo, mark, field, fashion, agen...</td>\n",
       "      <td>[(ford, 30), (field, 19), (``, 11), ('', 11), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  site                                              title              author  \\\n",
       "0  NBC  O.J. Simpson to Receive Parole Hearing in July...   Andrew Blankstein   \n",
       "1  NBC  Justice Department Narrows Possible Sanctions ...       Pete Williams   \n",
       "2  NBC  Daughter Pleads for Help in Search for Missing...       Rachael Trost   \n",
       "3  NBC  Alabama Death Row Inmate Tommy Arthur Pleads f...        Tracy Connor   \n",
       "4  NBC  Did Ford Nix Its CEO Because He Was Too Much o...  Paul A. Eisenstein   \n",
       "\n",
       "    secondary_authors published_on                accessed_on  \\\n",
       "0  ['Daniella Silva']         None 2017-05-23 09:33:29.288780   \n",
       "1                  []         None 2017-05-23 09:33:45.267153   \n",
       "2                  []         None 2017-05-23 09:33:54.960824   \n",
       "3                  []         None 2017-05-23 09:34:07.816292   \n",
       "4                  []         None 2017-05-23 09:34:47.090816   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://www.nbcnews.com/news/us-news/o-j-simpso...   \n",
       "1  http://www.nbcnews.com/news/us-news/justice-de...   \n",
       "2  http://www.nbcnews.com/feature/missing-in-amer...   \n",
       "3  http://www.nbcnews.com/storyline/lethal-inject...   \n",
       "4  http://www.nbcnews.com/business/autos/ford-s-f...   \n",
       "\n",
       "                                                body  \\\n",
       "0  After nearly a decade behind bars, O.J. Simpso...   \n",
       "1  Attorney General Jeff Sessions said Monday tha...   \n",
       "2  Clarissa is Lisa Harvey's only child.  They tw...   \n",
       "3  Seven times, Tommy Arthur has escaped death. W...   \n",
       "4  Former Ford CEO Mark Fields fashioned himself ...   \n",
       "\n",
       "                                                html  \\\n",
       "0  <div><p> After nearly a decade behind bars, O....   \n",
       "1  <div><p> Attorney General Jeff Sessions said M...   \n",
       "2  <div><p> Clarissa is Lisa Harvey's only child....   \n",
       "3  <div><p> Seven times, Tommy Arthur has escaped...   \n",
       "4  <div><p> Former Ford CEO Mark Fields fashioned...   \n",
       "\n",
       "                                  newspaper_keywords  \\\n",
       "0  {simpson,board,charges,nevada,oj,prison,receiv...   \n",
       "1  {order,possible,justice,narrows,sanctions,secu...   \n",
       "2  {lake,woman,phone,daughter,told,missing,lisas,...   \n",
       "3  {row,inmate,court,wicker,guilty,testing,arthur...   \n",
       "4  {visionary,hackett,vision,nix,ceo,chairman,for...   \n",
       "\n",
       "                                   newspaper_summary     id  \\\n",
       "0  Simpson is set to face a parole hearing this J...  75718   \n",
       "1  Attorney General Jeff Sessions said Monday tha...  75719   \n",
       "2  Lisa Harvey Help Find Lisa Gail Harvey Faceboo...  75720   \n",
       "3  Seven times, Tommy Arthur has escaped death.\\n...  75721   \n",
       "4  Former Ford CEO Mark Fields fashioned himself ...  75722   \n",
       "\n",
       "                                      tokenized_body  word_count  \\\n",
       "0  [After, nearly, a, decade, behind, bars, ,, O....         438   \n",
       "1  [Attorney, General, Jeff, Sessions, said, Mond...         366   \n",
       "2  [Clarissa, is, Lisa, Harvey, 's, only, child, ...         991   \n",
       "3  [Seven, times, ,, Tommy, Arthur, has, escaped,...        1221   \n",
       "4  [Former, Ford, CEO, Mark, Fields, fashioned, h...        1247   \n",
       "\n",
       "                                     stopworded_body  \\\n",
       "0  [after, nearly, decade, behind, bars, o.j, sim...   \n",
       "1  [attorney, general, jeff, sessions, said, mond...   \n",
       "2  [clarissa, lisa, harvey, child, they, two, inc...   \n",
       "3  [seven, times, tommy, arthur, escaped, death, ...   \n",
       "4  [former, ford, ceo, mark, fields, fashioned, a...   \n",
       "\n",
       "                                     lemmatized_body  \\\n",
       "0  [after, nearly, decade, behind, bar, o.j, simp...   \n",
       "1  [attorney, general, jeff, session, say, monday...   \n",
       "2  [clarissa, lisa, harvey, child, they, two, inc...   \n",
       "3  [seven, time, tommy, arthur, escape, death, wi...   \n",
       "4  [former, ford, ceo, mark, field, fashion, agen...   \n",
       "\n",
       "                                            word_bag  \n",
       "0  [(parole, 12), (simpson, 11), (charge, 5), (bo...  \n",
       "1  [(local, 6), (federal, 6), (city, 6), (session...  \n",
       "2  [(lisa, 26), (clarissa, 18), (say, 14), ('', 1...  \n",
       "3  [(arthur, 27), (``, 17), ('', 16), (say, 15), ...  \n",
       "4  [(ford, 30), (field, 19), (``, 11), ('', 11), ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopworded_body = []\n",
    "for body in df['tokenized_body']:\n",
    "    stopworded_body.append([w.lower() for w in body if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "se = pd.Series(stopworded_body)\n",
    "df['stopworded_body'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "lemmatized_body = []\n",
    "for body in df['stopworded_body']:\n",
    "    # We need to tag words with their parts of speech before the WordNet lemmatizer will work properly\n",
    "    pos_tagged_body = nltk.pos_tag(body)\n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos_tagged_body:\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        if not wntag:\n",
    "            lemma = word\n",
    "        else:\n",
    "            lemma = wnl.lemmatize(word, wntag)\n",
    "        lemmatized_words.append(lemma)\n",
    "    lemmatized_body.append(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "se = pd.Series(lemmatized_body)\n",
    "df['lemmatized_body'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_bag = []\n",
    "for body in df['lemmatized_body']:\n",
    "    fdist = FreqDist(body)\n",
    "    # FreqDist returns a special nltk.probability.FreqDist type\n",
    "    # This is a list of tuples\n",
    "    # Here is an example of how to access the elements for future reference\n",
    "#     print(fdist.most_common())\n",
    "    # Access an individual tuple\n",
    "#     print(fdist.most_common()[0])\n",
    "    # Access the word from the tuple\n",
    "#     print(fdist.most_common()[0][0])\n",
    "    # Access the count from the tuple\n",
    "#     print(fdist.most_common()[0][1])\n",
    "    # Append to list as ordered frequency distribution\n",
    "    word_bag.append(fdist.most_common())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "se = pd.Series(word_bag)\n",
    "df['word_bag'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st = StanfordNERTagger('/media/justin/Data/Google Drive/Assignments and Projects/Machine Learning/NLP/english.all.3class.distsim.crf.ser.gz',\n",
    "\t\t\t\t\t   '/media/justin/Data/Google Drive/Assignments and Projects/Machine Learning/NLP/stanford-ner.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classified_texts = []\n",
    "for body in df['tokenized_body']:\n",
    "    classified_texts.append(st.tag(body))\n",
    "\n",
    "# print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def stanfordNE2BIO(tagged_sent):\n",
    "    bio_tagged_sent = []\n",
    "    prev_tag = \"O\"\n",
    "    for token, tag in tagged_sent:\n",
    "        if tag == \"O\": #O\n",
    "            bio_tagged_sent.append((token, tag))\n",
    "            prev_tag = tag\n",
    "            continue\n",
    "        if tag != \"O\" and prev_tag == \"O\": # Begin NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag == tag: # Inside NE\n",
    "            bio_tagged_sent.append((token, \"I-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag != tag: # Adjacent NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "\n",
    "    return bio_tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stanfordNE2tree(ne_tagged_sent):\n",
    "    bio_tagged_sent = stanfordNE2BIO(ne_tagged_sent)\n",
    "    sent_tokens, sent_ne_tags = zip(*bio_tagged_sent)\n",
    "    sent_pos_tags = [pos for token, pos in pos_tag(sent_tokens)]\n",
    "\n",
    "    sent_conlltags = [(token, pos, ne) for token, pos, ne in zip(sent_tokens, sent_pos_tags, sent_ne_tags)]\n",
    "    ne_tree = conlltags2tree(sent_conlltags)\n",
    "    return ne_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ne_trees = []\n",
    "for text in classified_texts:\n",
    "    try:\n",
    "        ne_trees.append(stanfordNE2tree(text))\n",
    "    except:\n",
    "        ne_trees.append(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ne_in_sent = []\n",
    "ne_in_sents = []\n",
    "for tree in ne_trees:\n",
    "    ne_in_sent = []\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == Tree: # If subtree is a noun chunk, i.e. NE != \"O\"\n",
    "            ne_label = subtree.label()\n",
    "            ne_string = \" \".join([token for token, pos in subtree.leaves()])\n",
    "            ne_in_sent.append((ne_string, ne_label))\n",
    "    ne_in_sents.append(ne_in_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "se = pd.Series(ne_in_sents)\n",
    "df['named_entities'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lex_div = []\n",
    "for body in df['stopworded_body']:\n",
    "    lex_div.append(lexical_diversity(body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "se = pd.Series(lex_div)\n",
    "df['lexical_diversity'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | pattern u'/home/justin/GitHub/CapstoneI/config_template.ipynb' matched no files\r\n",
      "This application is used to convert notebook files (*.ipynb) to various other\r\n",
      "formats.\r\n",
      "\r\n",
      "WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\r\n",
      "\r\n",
      "Options\r\n",
      "-------\r\n",
      "\r\n",
      "Arguments that take values are actually convenience aliases to full\r\n",
      "Configurables, whose aliases are listed on the help line. For more information\r\n",
      "on full configurables, see '--help-all'.\r\n",
      "\r\n",
      "-y\r\n",
      "    Answer yes to any questions instead of prompting.\r\n",
      "--execute\r\n",
      "    Execute the notebook prior to export.\r\n",
      "--allow-errors\r\n",
      "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\r\n",
      "--stdout\r\n",
      "    Write notebook output to stdout instead of files.\r\n",
      "--debug\r\n",
      "    set log level to logging.DEBUG (maximize logging output)\r\n",
      "--inplace\r\n",
      "    Run nbconvert in place, overwriting the existing notebook (only \r\n",
      "    relevant when converting to notebook format)\r\n",
      "--generate-config\r\n",
      "    generate default config file\r\n",
      "--reveal-prefix=<Unicode> (SlidesExporter.reveal_url_prefix)\r\n",
      "    Default: u''\r\n",
      "    The URL prefix for reveal.js. This can be a a relative URL for a local copy\r\n",
      "    of reveal.js, or point to a CDN.\r\n",
      "    For speaker notes to work, a local reveal.js prefix must be used.\r\n",
      "--nbformat=<Enum> (NotebookExporter.nbformat_version)\r\n",
      "    Default: 4\r\n",
      "    Choices: [1, 2, 3, 4]\r\n",
      "    The nbformat version to write. Use this to downgrade notebooks.\r\n",
      "--writer=<DottedObjectName> (NbConvertApp.writer_class)\r\n",
      "    Default: 'FilesWriter'\r\n",
      "    Writer class used to write the  results of the conversion\r\n",
      "--log-level=<Enum> (Application.log_level)\r\n",
      "    Default: 30\r\n",
      "    Choices: (0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL')\r\n",
      "    Set the log level by value or name.\r\n",
      "--to=<CaselessStrEnum> (NbConvertApp.export_format)\r\n",
      "    Default: 'html'\r\n",
      "    Choices: [u'custom', u'html', u'latex', u'markdown', u'notebook', u'pdf', u'python', u'rst', u'script', u'slides']\r\n",
      "    The export format to be used.\r\n",
      "--template=<Unicode> (TemplateExporter.template_file)\r\n",
      "    Default: u''\r\n",
      "    Name of the template file to use\r\n",
      "--output=<Unicode> (NbConvertApp.output_base)\r\n",
      "    Default: ''\r\n",
      "    overwrite base name use for output files. can only be used when converting\r\n",
      "    one notebook at a time.\r\n",
      "--post=<DottedOrNone> (NbConvertApp.postprocessor_class)\r\n",
      "    Default: u''\r\n",
      "    PostProcessor class used to write the  results of the conversion\r\n",
      "--config=<Unicode> (JupyterApp.config_file)\r\n",
      "    Default: u''\r\n",
      "    Full path of a config file.\r\n",
      "\r\n",
      "To see all available configurables, use `--help-all`\r\n",
      "\r\n",
      "Examples\r\n",
      "--------\r\n",
      "\r\n",
      "    The simplest way to use nbconvert is\r\n",
      "    \r\n",
      "    > jupyter nbconvert mynotebook.ipynb\r\n",
      "    \r\n",
      "    which will convert mynotebook.ipynb to the default format (probably HTML).\r\n",
      "    \r\n",
      "    You can specify the export format with `--to`.\r\n",
      "    Options include ['custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides']\r\n",
      "    \r\n",
      "    > jupyter nbconvert --to latex mynotebook.ipynb\r\n",
      "    \r\n",
      "    Both HTML and LaTeX support multiple output templates. LaTeX includes\r\n",
      "    'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\r\n",
      "    can specify the flavor of the format used.\r\n",
      "    \r\n",
      "    > jupyter nbconvert --to html --template basic mynotebook.ipynb\r\n",
      "    \r\n",
      "    You can also pipe the output to stdout, rather than a file\r\n",
      "    \r\n",
      "    > jupyter nbconvert mynotebook.ipynb --stdout\r\n",
      "    \r\n",
      "    PDF is generated via latex\r\n",
      "    \r\n",
      "    > jupyter nbconvert mynotebook.ipynb --to pdf\r\n",
      "    \r\n",
      "    You can get (and serve) a Reveal.js-powered slideshow\r\n",
      "    \r\n",
      "    > jupyter nbconvert myslides.ipynb --to slides --post serve\r\n",
      "    \r\n",
      "    Multiple notebooks can be given at the command line in a couple of \r\n",
      "    different ways:\r\n",
      "    \r\n",
      "    > jupyter nbconvert notebook*.ipynb\r\n",
      "    > jupyter nbconvert notebook1.ipynb notebook2.ipynb\r\n",
      "    \r\n",
      "    or you can specify the notebooks list in a config file, containing::\r\n",
      "    \r\n",
      "        c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\r\n",
      "    \r\n",
      "    > jupyter nbconvert --config mycfg.py\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script /home/justin/GitHub/CapstoneI/config_template.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
