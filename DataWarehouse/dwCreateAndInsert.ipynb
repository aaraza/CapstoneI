{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load 'NLP Feature Extraction_DW_Edition.py'\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import re\n",
    "from __future__ import division\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "conn = psycopg2.connect(\"dbname='cap' user='postgres' host='ec2-35-163-99-253.us-west-2.compute.amazonaws.com' port=9000 password ='secret'\")\n",
    "df = pd.read_sql_query(\"SELECT * FROM articles limit 5\", conn)\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "tokenized_body = []\n",
    "for body in df['body']:\n",
    "    body = body.decode('utf-8')\n",
    "    tokens = nltk.word_tokenize(body)\n",
    "    tokenized_body.append(tokens)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "se = pd.Series(tokenized_body)\n",
    "df['tokenized_body'] = se.values\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "word_count = []\n",
    "for body in df['tokenized_body']:\n",
    "    word_count.append(len(body))\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "se = pd.Series(word_count)\n",
    "df['word_count'] = se.values\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + [',', '.', '!', '?', '\"','\\'', '/', '\\\\', '-', '--', 'â€”', '(', ')', '[', ']', '\\'s', '\\'t', '\\'ve', '\\'d', '\\'ll', '\\'re']\n",
    "stop_words = set(stop_words) # making this a set increases performance for large documents\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "df\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "stopworded_body = []\n",
    "for body in df['tokenized_body']:\n",
    "    stopworded_body.append([w.lower() for w in body if w not in stop_words])\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "se = pd.Series(stopworded_body)\n",
    "df['stopworded_body'] = se.values\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "lemmatized_body = []\n",
    "for body in df['stopworded_body']:\n",
    "    # We need to tag words with their parts of speech before the WordNet lemmatizer will work properly\n",
    "    pos_tagged_body = nltk.pos_tag(body)\n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos_tagged_body:\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        if not wntag:\n",
    "            lemma = word\n",
    "        else:\n",
    "            lemma = wnl.lemmatize(word, wntag)\n",
    "        lemmatized_words.append(lemma)\n",
    "    lemmatized_body.append(lemmatized_words)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "se = pd.Series(lemmatized_body)\n",
    "df['lemmatized_body'] = se.values\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "word_bag = []\n",
    "for body in df['lemmatized_body']:\n",
    "    fdist = FreqDist(body)\n",
    "    # FreqDist returns a special nltk.probability.FreqDist type\n",
    "    # This is a list of tuples\n",
    "    # Here is an example of how to access the elements for future reference\n",
    "#     print(fdist.most_common())\n",
    "    # Access an individual tuple\n",
    "#     print(fdist.most_common()[0])\n",
    "    # Access the word from the tuple\n",
    "#     print(fdist.most_common()[0][0])\n",
    "    # Access the count from the tuple\n",
    "#     print(fdist.most_common()[0][1])\n",
    "    # Append to list as ordered frequency distribution\n",
    "    word_bag.append(fdist.most_common())\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "se = pd.Series(word_bag)\n",
    "df['word_bag'] = se.values\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "st = StanfordNERTagger('/media/justin/Data/Google Drive/Assignments and Projects/Machine Learning/NLP/english.all.3class.distsim.crf.ser.gz',\n",
    "\t\t\t\t\t   '/media/justin/Data/Google Drive/Assignments and Projects/Machine Learning/NLP/stanford-ner.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "classified_texts = []\n",
    "for body in df['tokenized_body']:\n",
    "    classified_texts.append(st.tag(body))\n",
    "\n",
    "# print(classified_text)\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def stanfordNE2BIO(tagged_sent):\n",
    "    bio_tagged_sent = []\n",
    "    prev_tag = \"O\"\n",
    "    for token, tag in tagged_sent:\n",
    "        if tag == \"O\": #O\n",
    "            bio_tagged_sent.append((token, tag))\n",
    "            prev_tag = tag\n",
    "            continue\n",
    "        if tag != \"O\" and prev_tag == \"O\": # Begin NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag == tag: # Inside NE\n",
    "            bio_tagged_sent.append((token, \"I-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag != tag: # Adjacent NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "\n",
    "    return bio_tagged_sent\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "def stanfordNE2tree(ne_tagged_sent):\n",
    "    bio_tagged_sent = stanfordNE2BIO(ne_tagged_sent)\n",
    "    sent_tokens, sent_ne_tags = zip(*bio_tagged_sent)\n",
    "    sent_pos_tags = [pos for token, pos in pos_tag(sent_tokens)]\n",
    "\n",
    "    sent_conlltags = [(token, pos, ne) for token, pos, ne in zip(sent_tokens, sent_pos_tags, sent_ne_tags)]\n",
    "    ne_tree = conlltags2tree(sent_conlltags)\n",
    "    return ne_tree\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "ne_trees = []\n",
    "for text in classified_texts:\n",
    "    try:\n",
    "        ne_trees.append(stanfordNE2tree(text))\n",
    "    except:\n",
    "        ne_trees.append(' ')\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "ne_in_sent = []\n",
    "ne_in_sents = []\n",
    "for tree in ne_trees:\n",
    "    ne_in_sent = []\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == Tree: # If subtree is a noun chunk, i.e. NE != \"O\"\n",
    "            ne_label = subtree.label()\n",
    "            ne_string = \" \".join([token for token, pos in subtree.leaves()])\n",
    "            ne_in_sent.append((ne_string, ne_label))\n",
    "    ne_in_sents.append(ne_in_sent)\n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "se = pd.Series(ne_in_sents)\n",
    "df['named_entities'] = se.values\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text) * 100\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "lex_div = []\n",
    "for body in df['stopworded_body']:\n",
    "    lex_div.append(lexical_diversity(body))\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "se = pd.Series(lex_div)\n",
    "df['lexical_diversity'] = se.values\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "# get_ipython().system(u'jupyter nbconvert --to script /home/justin/GitHub/CapstoneI/config_template.ipynb')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'site', u'title', u'author', u'secondary_authors', u'published_on',\n",
       "       u'accessed_on', u'url', u'body', u'html', u'newspaper_keywords',\n",
       "       u'newspaper_summary', u'id', u'tokenized_body', u'word_count',\n",
       "       u'stopworded_body', u'lemmatized_body', u'word_bag', u'named_entities',\n",
       "       u'lexical_diversity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname='cap' user='postgres' host='ec2-35-163-99-253.us-west-2.compute.amazonaws.com' port=9000 password ='secret'\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('drop table if exists author_dim cascade')\n",
    "cursor.execute('create table author_dim(author_id SERIAL,name TEXT,PRIMARY KEY(author_id))')\n",
    "\n",
    "cursor.execute('drop table if exists site_dim cascade')\n",
    "cursor.execute('create table site_dim(site_id SERIAL,domain text,url text,supersite_id int,PRIMARY KEY(site_id))')\n",
    "\n",
    "cursor.execute('drop table if exists article_dim cascade')\n",
    "cursor.execute('create table article_dim(article_id SERIAL, title text, published_date timestamp,accessed_date timestamp, body text,html text, keywords text, summary text,PRIMARY KEY(article_id))')\n",
    "\n",
    "cursor.execute('drop table if exists statistic_dim cascade')\n",
    "cursor.execute('create table statistic_dim(statistic_id SERIAL,NLP_results JSON,PRIMARY KEY(statistic_id))')\n",
    "\n",
    "cursor.execute('drop table if exists fact cascade')\n",
    "cursor.execute('create table fact(fact_id SERIAL,article_id int,author_id int,site_id int,statistic_id int,FOREIGN KEY (article_id) REFERENCES article_dim(article_id),FOREIGN KEY (author_id) REFERENCES author_dim(author_id),FOREIGN KEY (site_id) REFERENCES site_dim(site_id),FOREIGN KEY (statistic_id) REFERENCES statistic_dim(statistic_id))')\n",
    "\n",
    "cursor.execute('drop table if exists nlp_dim cascade')\n",
    "cursor.execute(\"create table nlp_dim(article_id SERIAL,tokenized_body text, word_count integer, stopworded_body text, lemmatized_body text, word_bag text, named_entities text, lexical_diversity float, PRIMARY KEY(article_id))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "article_table = df[['id','title', 'published_on','accessed_on', 'body', 'html','newspaper_keywords', 'newspaper_summary']]\n",
    "article_table.accessed_on = article_table.accessed_on[0].split(' ')[0]\n",
    "article_table.columns = ['article_id','title', 'published_date','accessed_date', 'body', 'html','keywords', 'summary']\n",
    "article_table\n",
    "#engine = create_engine('postgresql://postgres:secret@ec2-35-163-99-253.us-west-2.compute.amazonaws.com:9000/cap')\n",
    "#article_table.to_sql('article_dim', engine, if_exists='append', index=None)\n",
    "# df[['id','tokenized_body', 'word_count','stopworded_body', 'lemmatized_body','word_bag','named_entities','lexical_diversity']].to_sql('nlp_dim', engine, if_exists='append', index=None)\n",
    "# cursor.execute('select * from article_dim limit 1')\n",
    "# cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp_table = df[['id','tokenized_body', 'word_count','stopworded_body', 'lemmatized_body','word_bag','named_entities','lexical_diversity']]\n",
    "nlp_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp.to_sql('nlp_dim', engine, if_exists='append', index=None)\n",
    "cursor.execute('select * from nlp_dim limit 1')\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conn = psycopg2.connect(\"dbname='cap' user='postgres' host='ec2-35-163-99-253.us-west-2.compute.amazonaws.com' port=9000 password ='secret'\")\n",
    "# cursor = conn.cursor()\n",
    "# cursor.execute('drop table if exists author_dim cascade')\n",
    "# cursor.execute('create table author_dim(author_id SERIAL,name TEXT,PRIMARY KEY(author_id))')\n",
    "\n",
    "# cursor.execute('drop table if exists site_dim cascade')\n",
    "# cursor.execute('create table site_dim(site_id SERIAL,domain text,url text,supersite_id int,PRIMARY KEY(site_id))')\n",
    "\n",
    "# cursor.execute('drop table if exists article_dim cascade')\n",
    "# cursor.execute('create table article_dim(article_id SERIAL, title text, published_date timestamp,accessed_date timestamp, body text,html text, keywords text, summary text,PRIMARY KEY(article_id))')\n",
    "\n",
    "# cursor.execute('drop table if exists statistic_dim cascade')\n",
    "# cursor.execute('create table statistic_dim(statistic_id SERIAL,NLP_results JSON,PRIMARY KEY(statistic_id))')\n",
    "\n",
    "# cursor.execute('drop table if exists fact cascade')\n",
    "# cursor.execute('create table fact(fact_id SERIAL,article_id int,author_id int,site_id int,statistic_id int,FOREIGN KEY (article_id) REFERENCES article_dim(article_id),FOREIGN KEY (author_id) REFERENCES author_dim(author_id),FOREIGN KEY (site_id) REFERENCES site_dim(site_id),FOREIGN KEY (statistic_id) REFERENCES statistic_dim(statistic_id))')\n",
    "\n",
    "# cursor.execute('drop table if exists nlp_dim cascade')\n",
    "cursor.execute(\"select * from nlp_dim limit 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_table = pd.DataFrame(data=None, columns=['author_id', 'author'], index = None)\n",
    "author_table.author = data['author'].unique()\n",
    "author_table.author_id = author_table.index \n",
    "author_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_tableMM = data[['author', 'id']]\n",
    "author_tableMM.columns = ['author', 'article_id']\n",
    "author_tableMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.merge(author_table, author_tableMM, on='author')\n",
    "result[result.article_id==2910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_table[author_table.author_id==507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s= data['url'][0]\n",
    "#print(s)\n",
    "s = s.replace(\"http://www.\",\"\")\n",
    "#print(s)\n",
    "domain = s.rsplit('/')[0]\n",
    "#print(domain)\n",
    "supersite = domain.rsplit('.')[0]\n",
    "print(s, domain, supersite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site_table = pd.DataFrame(data=data[['id','url']], columns=['id','site_id', 'url', 'domain', 'supersite'], index = None)\n",
    "site_table.url = data['url']\n",
    "site_table.url = site_table.url.str.replace(\"http://www.\",\"\")\n",
    "site_table.domain = site_table.url.apply(lambda x: pd.Series(str(x).split('/',1)))\n",
    "site_table.supersite = site_table.domain.apply(lambda x: pd.Series(str(x).split('.',1)))\n",
    "site_table.site_id = site_table.index\n",
    "site_table.columns = ['article_id','site_id', 'URL', 'domain', 'supersite']\n",
    "site_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result1 = pd.merge(author_tableMM, author_table, on='author')[['article_id','author_id']]\n",
    "result2 = pd.merge(site_table, result1, on='article_id')[['article_id','author_id','site_id']]\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import re\n",
    "pattern = re.compile('(^[^.]*[.])([^.]*)([.].*$)')\n",
    "site_table = pd.DataFrame(data=data[['id','url']], columns=['id','site_id', 'url', 'domain', 'supersite'], index = None)\n",
    "site_table.url = data['url']\n",
    "#site_table.url =  urlparse(site_table.url).netloc\n",
    "for x in site_table.url:\n",
    "    x = urlparse(x).netloc\n",
    "\n",
    "\n",
    "'''site_table.domain = site_table.url.apply(lambda x: pd.Series(str(x).split('/',1)))\n",
    "site_table.supersite = site_table.domain.apply(lambda x: pd.Series(str(x).split('.',1)))\n",
    "site_table.site_id = site_table.index\n",
    "site_table.columns = ['article_id','site_id', 'URL', 'domain', 'supersite']\n",
    "site_table'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import re\n",
    "pattern = re.compile('(^[^.]*[.])([^.]*)([.].*$)')\n",
    "a = urlparse(data.url[0]).netloc\n",
    "print(a)\n",
    "b = [pattern.sub(r'\\2', x) for x in [a]][0]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = urlparse(data.url[0])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
