{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be used when converted to real script to maintain ID ordering when we cluster and label \n",
    "# just need to change target table \n",
    "\n",
    "conn = psycopg2.connect(\"dbname='cap' user='postgres' host='ec2-52-27-114-159.us-west-2.compute.amazonaws.com' port=9000 password ='secret'\")\n",
    "data = pd.read_sql_query(\"SELECT * FROM nlp_dim ORDER BY id ASC\", conn)\n",
    "\n",
    "\n",
    "# going to try on a bunch of article bodies without NLP for performance\n",
    "# data = pd.read_sql_query(\"SELECT * FROM articles ORDER BY id ASC\", conn)\n",
    "\n",
    "# data = pd.read_csv('nlp_dim_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 164498 entries, 0 to 164497\n",
      "Data columns (total 10 columns):\n",
      "site                  164498 non-null object\n",
      "title                 164495 non-null object\n",
      "author                126451 non-null object\n",
      "published_on          130732 non-null object\n",
      "accessed_on           164498 non-null datetime64[ns]\n",
      "url                   164498 non-null object\n",
      "body                  164498 non-null object\n",
      "newspaper_keywords    164498 non-null object\n",
      "newspaper_summary     164498 non-null object\n",
      "id                    164498 non-null int64\n",
      "dtypes: datetime64[ns](1), int64(1), object(8)\n",
      "memory usage: 12.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/tf_vectorizer_obj.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "# transforms data into tfidf matrix representation\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=100,\n",
    "                                 min_df=2, use_idf=True)\n",
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "\n",
    "joblib.dump(vectorizer, 'model/tf_vectorizer_obj.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<164498x100 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5169957 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit our data (list of article bodies) to a tfidf representation\n",
    "X = vectorizer.fit_transform(data.body)\n",
    "\n",
    "# verify we have a sparse matrix of 100 tfidf features for each article \n",
    "# should be 5*100 sparse matrix\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data that we have of TFIDF vectors into a file\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('model/tf_idf.npz', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<164498x100 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5169957 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sparse.load_npz('model/tf_idf.npz')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# How many clusters we want\n",
    "true_k = 3\n",
    "\n",
    "# create the KMeans object with initial settings\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 224909.159\n",
      "Iteration  1, inertia 126097.040\n",
      "Iteration  2, inertia 122803.882\n",
      "Iteration  3, inertia 121398.927\n",
      "Iteration  4, inertia 120285.605\n",
      "Iteration  5, inertia 119855.714\n",
      "Iteration  6, inertia 119659.509\n",
      "Iteration  7, inertia 119540.634\n",
      "Iteration  8, inertia 119517.138\n",
      "Iteration  9, inertia 119513.315\n",
      "Iteration 10, inertia 119512.710\n",
      "Iteration 11, inertia 119512.506\n",
      "Iteration 12, inertia 119512.446\n",
      "Converged at iteration 12: center shift 4.437656e-07 within tolerance 8.089938e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=3, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit our tfidf data to the kmeans model\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '2017', 'according', 'against', 'any', 'around', 'back', 'because', 'before', 'being', 'between', 'company', 'continue', 'could', 'day', 'de', 'did', 'do', 'don', 'down', 'during', 'even', 'first', 'game', 'get', 'go', 'going', 'good', 'government', 'her', 'here', 'him', 'home', 'house', 'how', 'know', 'last', 'long', 'made', 'main', 'make', 'many', 'may', 'me', 'million', 'most', 'mr', 'much', 'my', 'news', 'no', 'now', 'off', 'only', 'our', 'over', 'people', 'president', 're', 'reading', 'right', 'say', 'says', 'see', 'she', 'should', 'since', 'state', 'still', 'story', 'such', 'take', 'them', 'then', 'these', 'think', 'those', 'three', 'through', 'times', 'told', 'trump', 'twitter', 'two', 'us', 've', 'very', 'want', 'way', 'week', 'well', 'where', 'while', 'white', 'work', 'world', 'year', 'years', 'york', 'your']\n",
      " trump president house white news\n",
      "\n",
      " your company people year our\n",
      "\n",
      " her she de my people\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save kmeans model \n",
    "joblib.dump(km, 'model/kmeans_model.pkl')\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "labels = km.labels_\n",
    "print(terms)\n",
    "\n",
    "# order_centroids\n",
    "\n",
    "for i in range(3):\n",
    "    for ind in order_centroids[i, :5]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trump president house white news\n",
      "\n",
      " your company people year our\n",
      "\n",
      " her she de my people\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare saved and loaded kmeans\n",
    "kmeans_loaded = joblib.load('model/kmeans_model.pkl')\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "order_centroids = kmeans_loaded.cluster_centers_.argsort()[:, ::-1]\n",
    "labels = kmeans_loaded.labels_\n",
    "\n",
    "# order_centroids\n",
    "\n",
    "for i in range(3):\n",
    "    for ind in order_centroids[i, :5]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small test for how we can eventually persist the cluster labels for individual articles\n",
    "# Since the labels attribute is in the order that the sparse matrix was in when it was passed in\n",
    "# We should be able just insert the label value as a dataframe column\n",
    "\n",
    "t = pd.Series(labels)\n",
    "data['cluster_label'] = t\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=100)\n",
    "X_test = tfidf.fit_transform([data.lemmatized_body[98]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = km.predict(X_test)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
